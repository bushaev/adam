{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't actually need the function, we just need its gradient\n",
    "def grad(t, C=15):\n",
    "    if x % 3 == 1:\n",
    "        return C\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We look at value in [-1, 1], because the function can always be optimized better\n",
    "def project(x):\n",
    "    if -1 <= x <= 1:\n",
    "        return x\n",
    "    elif x <= -1:\n",
    "        return -1\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 10000\n",
    "C = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic gradient descent \n",
    "### converges to optimal value of -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0\n",
    "lr = 0.0001\n",
    "for t in range(num_iterations):\n",
    "    g_t = grad(t, C)\n",
    "    x = project(x - lr * g_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.9998"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam\n",
    "### With specific values of beta_1 and beta_2\n",
    "### converges to sub-optimal point of 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that with default values beta_1 = 0.9 and beta_2 = 0.999 Adam converges to -1 for this exact example with C = 15.\n",
    "Large values of epsilon (>100) also gets Adam to converge to optimal point -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_t = 0\n",
    "v_t = 0\n",
    "epsilon = 1e-7\n",
    "x = 0\n",
    "beta_1 = 0.0\n",
    "beta_2 = 1 / (1 + C * C )\n",
    "step_size = 0.1\n",
    "v_hat = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(num_iterations):\n",
    "    \n",
    "    g_t = grad(t, C)\n",
    "    m_t = beta_1 * m_t + (1 - beta_1) * g_t\n",
    "    v_t = beta_2 * v_t + (1 - beta_2) * (g_t * g_t)\n",
    "    m_hat = m_t / (1 - np.power(beta_1, t + 1))\n",
    "    v_hat = v_t / (1 - np.power(beta_2, t + 1))\n",
    "    \n",
    "    x = project(x - step_size * m_hat / (v_hat + epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AMSGRAD converges to optimal point\n",
    "## without the need to modify hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_t = 0\n",
    "v_t = 0\n",
    "epsilon = 1e-7\n",
    "x = 0\n",
    "beta_1 = 0.0\n",
    "beta_2 = 1 / (1 + C * C )\n",
    "step_size = 0.1\n",
    "v_hat = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(num_iterations):\n",
    "    \n",
    "    g_t = grad(t, C)\n",
    "    m_t = beta_1 * m_t + (1 - beta_1) * g_t\n",
    "    v_t = beta_2 * v_t + (1 - beta_2) * (g_t * g_t)\n",
    "    v_hat = np.maximum(v_hat, v_t)\n",
    "    \n",
    "    x = project(x - step_size * m_t / (v_hat + epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.9991071782072143"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
